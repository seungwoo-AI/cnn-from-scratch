# CNNâ€‘fromâ€‘Scratch

> **A minimal NumPyâ€‘only CNN for 6â€¯Ã—â€¯6 digit recognition (1,â€¯2,â€¯3)** â€” built as the final project for *Numerical Optimization 2025*.
>
> ğŸ“„ *Full derivations, code listings, and discussion are in* `docs/FinalÂ Projectâ€¯2025.pdf`.

---

## Why this repo might help you

* **Walkâ€‘through code** â€” every layer, gradient, and update rule sits in a plain Python function you can singleâ€‘step in a debugger.
* **Frameworkâ€‘free** â€” no PyTorch / TensorFlow; just the standard library + NumPy.
* **Selfâ€‘contained dataset** â€” 96 tiny 6â€¯Ã—â€¯6 bitmaps live in one Excel sheet, so a full trainâ€‘eval run finishes in secondsÂ î˜•Â îˆ€fileciteîˆ‚turn0file0îˆ.
* **Six optimizers + three activations** ready to mix & match: SGD Â· Momentum Â· NAG Â· RMSProp Â· Adam Â· AdamW + ReLU Â· Sigmoid Â· Softmax.
* **Readable scripts** â€” one command trains, another plots the learning curve.

---

## Quick start

```bash
# set up (conda, venv â€¦)
python -m pip install -r requirements.txt  # numpy, pandas, matplotlib

# train SimpleCNN with Adam for 300 epochs
python code/train_adam.py --epochs 300 --lr 0.001

# visualise loss / accuracy
python code/plot_curve.py --log runs/adam-<timestamp>.log
```

*See `--help` on any `train_*.py` script for flags such as batch size, seed, or model variant.*

---

## Repository layout (oneâ€‘screen glance)

```text
code/              core source
 â”œâ”€ nn/            lowâ€‘level ops
 â”‚   â”œâ”€ conv.py          # Conv2D forward / backward (5â€‘loop NumPy)
 â”‚   â”œâ”€ activation.py    # ReLU Â· Sigmoid Â· Softmaxâ€‘CE
 â”‚   â””â”€ optimizer.py     # SGD â†” AdamW â€” 6 algorithms
 â”œâ”€ models/        network builders
 â”‚   â””â”€ model.py        # SimpleCNN & EnhancedCNN
 â”œâ”€ utils/         helpers
 â”‚   â””â”€ util_excel.py   # Excel â†’ (N,1,6,6) loader
 â”œâ”€ train_*.py     runnable experiments (one per optimiser)
 â””â”€ plot_curve.py  matplotlib learningâ€‘curve helper

data/handwriting_dataset.xlsx   96 labelled samples
LICENSE
README.md   (this file)
```

---

## How the pieces fit

1. **util\_excel.py** loads the Excel sheet into `(N,1,6,6)` float32 images + integer labels.
2. **model.py** assembles either:

   * *SimpleCNN* â†’ `Convâ€¯(3Ã—3) â†’ Sigmoid â†’ MaxPoolâ€¯(2Ã—2) â†’ Conv â†’ Sigmoid â†’ Softmax`.
   * *EnhancedCNN* â†’ 3Ã— `Convâ€¯+â€¯ReLU`, no pooling.
3. **optimizer.py** updates parameters via the algorithm you choose.
4. **train\_\*.py** drives the loop: forward â†’ loss â†’ backward â†’ update, for 300 epochs by default.
5. **plot\_curve.py** turns the run log into a PNG for your report.

Every forward / backward call returns explicit NumPy arrays so the math lines up with the equations in *AppendixÂ A* of the report.

---

## For reviewers / instructors

* **Mathematical proof** â€” AppendixÂ A shows backâ€‘prop equations for Softmaxâ€‘CE, ReLU, Sigmoid, MaxPool, and Conv2D, each paired with the matching code line numbers.
* **Reproducibility** â€” `train_all.py --seed 0â€‘9` reproduces every experiment table in the report.
* **Extensibility** â€” new layers drop in by subclassing the simple `Layer` skeleton; see `nn/activation.py` for a template.

---

## Next steps (roadmap)

* Batchâ€‘norm & dropout layers.
* Cython / Numba speedâ€‘ups for the 5â€‘loop convolution.
* Port the loader to MNIST or CIFARâ€‘10 (just swap `util_excel.py`).

---

Â©Â 2025 Seungâ€‘Woo Lee â€” MIT License
