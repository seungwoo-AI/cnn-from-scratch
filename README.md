# CNNâ€‘fromâ€‘Scratch ğŸ§‘â€ğŸ’»ğŸ“

Pureâ€‘Python (NumPyâ€‘only) Convolutional Neural Network that classifies **6â€¯Ã—â€¯6 handwritten digits (classesÂ :Â 1â€¯,â€¯2â€¯,â€¯3)** â€” implemented as the final project for the *Numerical Optimization* course.

This repository is meant to be **educational first**: every layer, gradient, and update rule is written from scratch so you can step through the code and *really* see what happens under the hood.

## âœ¨Â Key Points

* **Zero deepâ€‘learning frameworks** â€“ only the PythonÂ standard library,â€¯NumPy,â€¯Pandas,â€¯andâ€¯Matplotlib.
* **Multiple optimizers** â€“ SGD, RMSProp, Adam â€” switchable from the CLI.
* **Tiny custom dataset** â€“ an Excel file with 6Ã—6 bitmaps that lets the model train in seconds on a laptop CPU.
* **Clean experiment scripts** â€“ oneâ€‘liner training commands plus an automatic learningâ€‘curve visualizer.
* **Readable architecture** â€“ <100â€¯LOC for the CNN itself; everything is broken into small, testable pieces.

## ğŸ—‚ï¸Â Directory Layout

```text
.
â”œâ”€â”€ code
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â””â”€â”€ model.py            # 1â€‘Convâ€‘layer + 2â€‘FC CNN
â”‚   â”œâ”€â”€ nn
â”‚   â”‚   â”œâ”€â”€ conv.py             # forward / backward of 2â€‘D convolution
â”‚   â”‚   â”œâ”€â”€ activation.py       # ReLU, Softmax
â”‚   â”‚   â””â”€â”€ optimizer.py        # SGD, RMSProp, Adam (NumPy)
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â””â”€â”€ util_excel.py       # Excel â†” NumPy converter
â”‚   â”œâ”€â”€ train_sgd.py            # train with vanilla SGD
â”‚   â”œâ”€â”€ train_RMS.py            # train with RMSProp
â”‚   â”œâ”€â”€ train_adam.py           # train with Adam
â”‚   â”œâ”€â”€ train_all.py            # run a sweep over hyperâ€‘params
â”‚   â””â”€â”€ plot_curve.py           # loss / accuracy curves
â”œâ”€â”€ data
â”‚   â””â”€â”€ handwriting_dataset.xlsx # 6Ã—6 digit bitmaps & labels
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md   â† you are here
```

## ğŸš€Â Quick Start

```bash
# 1. Create an environment (conda or venv)
pip install -r requirements.txt  # only numpy, pandas, matplotlib

# 2. Train with your favourite optimizer
python code/train_adam.py --epochs 100 --lr 0.001

# 3. Plot learning curves
python code/plot_curve.py --log runs/adamâ€‘2025â€‘06â€‘11â€‘12â€‘34.log
```

## ğŸ§®Â Math Primer

The convolution used in `conv.py` is the discrete crossâ€‘correlation commonly employed in CNNs:

$$
y_{i,j,k}^{(l)} \;=\;
\\sigma\\Bigl(\\;\\sum_{c=1}^{C_{\\text{in}}}
\\bigl(W_{k}^{(l)} * x\\bigr)_{i,j,c}
\;+\; b_{k}^{(l)}\\Bigr)\\!,
$$

where $*$ denotes the 2â€‘D correlation operator and $\\sigma$ is ReLU.
The network is trained by minimising the crossâ€‘entropy

$$
\\mathcal{L}
= -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{c=1}^{3}
 y_{n,c}\\,\\log \\hat{y}_{n,c}.
$$

## ğŸ“ŠÂ Sample Results

| Optimizer | Final Accuracy (val) | Epochs |
| --------- | -------------------- | ------ |
| SGD       | 91â€¯%                 | 200    |
| RMSProp   | 94â€¯%                 | 150    |
| Adam      | **97â€¯%**             | 120    |

*(On the provided dataset; see `runs/` for complete logs.)*

## ğŸ“Â Dataset

`handwriting_dataset.xlsx` contains *N = 1â€¯800* grayscale 6Ã—6 bitmaps laid out rowâ€‘wise plus the groundâ€‘truth label in the last column.
The helper `util_excel.py` converts the sheet to a `(N, 6, 6)` NumPy array onâ€‘theâ€‘fly.

## ğŸ”¬Â How It Works â€“ Fileâ€‘byâ€‘File

| File            | Purpose                                                                 |
| --------------- | ----------------------------------------------------------------------- |
| `conv.py`       | Implements im2col + GEMM convolution and its backward pass              |
| `activation.py` | ReLU and numerically stable Softmax                                     |
| `optimizer.py`  | Plain NumPy implementations of SGD, RMSProp, Adam                       |
| `model.py`      | Composes `Conv2D â†’ ReLU â†’ FC â†’ Softmax`                                 |
| `train_*.py`    | Training loops with CLI flags (epochs, lr, batch size)                  |
| `plot_curve.py` | Reads the `.log` files produced during training and plots loss/accuracy |

Feel free to dive into any of themâ€”each function is thoroughly docâ€‘commented.

## ğŸ› ï¸Â Extending the Project

* Add more layers (e.g. MaxPool, Dropout).
* Swap the Excel dataset for **MNIST** (just change the loader).
* Implement **momentum** or **Nesterov** in `optimizer.py`.
* Port the training loop to **JAX** or **PyTorch** for comparison.

## ğŸ¤Â Contributing

Issues and pull requests are welcome! Fork the repo, create a feature branch, and open a PR.

## ğŸ“„Â License

This project is released under the MIT License â€“ see [LICENSE](LICENSE) for details.
