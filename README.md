# CNNâ€‘fromâ€‘Scratch&#x20;

> **A minimal NumPyâ€‘only CNN for 6â€¯Ã—â€¯6 digit recognition (1â€¯|â€¯2â€¯|â€¯3)** â€” final project for *Numerical OptimizationÂ 2025*.
>
> ðŸ“„ *Full math derivations and extended discussion:* see `docs/FinalÂ Projectâ€¯2025.pdf`.

---

## Why this repo might help you

* **Walkâ€‘through code** â€“ every layer, gradient, and update rule lives in a plain Python function you can step through in a debugger.
* **Frameworkâ€‘free** â€“ no PyTorchâ€¯/â€¯TensorFlow; just the standard library + NumPy.
* **Selfâ€‘contained dataset** â€“ 96 tiny 6â€¯Ã—â€¯6 bitmaps bundled in one Excel sheet; a full trainâ€‘eval run finishes in seconds.
* **Six optimizers Â· three activations** ready to mixÂ &Â match: *SGD Â· Momentum Â· NAG Â· RMSProp Â· Adam Â· AdamW* Â Ã—Â  *ReLU Â· Sigmoid Â· Softmax*.
* **Readable CLI scripts** â€“ one command trains, another plots the learning curve.

---

## Quick start (tested on PythonÂ 3.11 + NumPyÂ 1.26)

```bash
# create an environment (conda / venv)
python -m pip install -r requirements.txt  # numpy pandas matplotlib

# train SimpleCNN with Adam for 300 epochs
python code/train_adam.py --epochs 300 --lr 0.001
# âžœ runs/adam-<timestamp>.log is created automatically

# visualise loss & accuracy
python code/plot_curve.py --log runs/adam-<timestamp>.log
```

*Use **`--help`** on any **`train_*.py`** script for flags such as batch size, seed, model variant.*

---

## Dataset

`data/handwriting_dataset.xlsx` contains **96 greyscale 6â€¯Ã—â€¯6 bitmaps** and their labels.

| Split | #Â samples |
| ----- | --------- |
| Train | 72        |
| Val   | 12        |
| Test  | 12        |

`util_excel.py` converts each row into a `float32 (1,Â 6,Â 6)` tensor and the rightâ€‘most column into an integer label (1Â |Â 2Â |Â 3).

---

## Repository layout (oneâ€‘screen glance)

```text
code/              core source
 â”œâ”€ nn/            lowâ€‘level ops
 â”‚   â”œâ”€ conv.py          # Conv2D forward / backward (5â€‘loop NumPy)
 â”‚   â”œâ”€ activation.py    # ReLU Â· Sigmoid Â· Softmaxâ€‘CE
 â”‚   â””â”€ optimizer.py     # SGD â†” AdamW â€” 6 algorithms
 â”œâ”€ models/        network builders
 â”‚   â””â”€ model.py        # SimpleCNN & EnhancedCNN
 â”œâ”€ utils/         helpers
 â”‚   â””â”€ util_excel.py   # Excel â†’ (N,1,6,6) loader
 â”œâ”€ train_*.py     runnable experiments (one per optimiser)
 â””â”€ plot_curve.py  matplotlib learningâ€‘curve helper

data/handwriting_dataset.xlsx   96 labelled samples
LICENSE
README.md   (this file)
```

---

## How the pieces fit

1. **util\_excel.py** loads the Excel sheet â†’ `(N,Â 1,Â 6,Â 6)` images + integer labels.
2. **model.py** assembles either:

   * *SimpleCNN* â†’ `Convâ€¯(3Ã—3) â†’ Sigmoid â†’ MaxPoolâ€¯(2Ã—2) â†’ Conv â†’ Sigmoid â†’ Softmax`.
   * *EnhancedCNN* â†’ 3Ã— `Conv + ReLU`, no pooling.
3. **optimizer.py** updates parameters via the chosen algorithm.
4. **train\_\*.py** : forward â†’ loss â†’ backward â†’ update (default 300 epochs).
5. **plot\_curve.py** : turn the run log into a PNG for your report.

All tensors are explicit NumPy arrays â€“ equations in *AppendixÂ A* map 1â€‘toâ€‘1 to code line numbers.

---

## For reviewers / instructors

* **Mathematical proof** â€“ Appendixâ€¯A shows backâ€‘prop equations for Conv2D, MaxPool, ReLU, Sigmoid, Softmaxâ€‘CE with code references.
* **Reproducibility** â€“ `train_all.py --seed 0â€‘9` regenerates every experiment table in the report.
* **Extensibility** â€“ new layers drop in by subclassing the tiny `Layer` skeleton; see `nn/activation.py` for a template.
* **Code style** â€“ PEPÂ 8 compliant; formatted with `black` 24.3.

---

## Roadmap

* Batchâ€‘norm & dropout layers.
* Cython / Numba speedâ€‘ups for the 5â€‘loop convolution.
* Port the loader to MNIST or CIFARâ€‘10 (just swapÂ `util_excel.py`).

---

Â©Â 2025Â Seungâ€‘WooÂ Lee â€” MIT License
